{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰（パーセプトロン）による記事ジャンルの分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練・テストデータの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "path_to_corpus = 'text' # livedoor ニュースコーパスのフォルダを指定してください．\n",
    "sports_list = glob(path_to_corpus + '/sports-watch/sports-watch*.txt')\n",
    "others_list = [p for p in glob(path_to_corpus + '/*/*.txt') if 'sports-watch' not in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "def write_files(out, filenames, label):\n",
    "    for f in filenames:\n",
    "        out.write(' '.join(l.strip().replace('\\t', ' ') for l in open(f).readlines()[2:]))\n",
    "        out.write('\\t' + label + \"\\n\")\n",
    "\n",
    "out_train = open('train.tsv', 'w')\n",
    "out_test = open('test.tsv', 'w')\n",
    "sports_files = random.sample(sports_list, 500)\n",
    "others_files = random.sample(others_list, 500)\n",
    "\n",
    "write_files(out_train, sports_files[:400], 'sports')\n",
    "write_files(out_train, others_files[:400], 'others')\n",
    "write_files(out_test, sports_files[400:], 'sports')\n",
    "write_files(out_test, others_files[400:], 'others')\n",
    "\n",
    "out_train.close()\n",
    "out_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練データの読み込みとモデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】安藤美姫“日本の人の声は気になったり、今でも怖い” 日本テレビ「バンクーバー2010」（27日放送）には女子フィギュアスケートで活躍した安藤美姫＆鈴木明子が出演、大舞台での演技から一夜明け、その心境を語った。  「トリノと比べると、落ち着いて一日一日を過ごせて、メダルが目標だったので、残念だったんですけど、演技自体はオリンピックの舞台でミスなく終われて幸せでした。スケートやっててよかったな。初めて心から幸せだなと思えた」という安藤に、パーソナルベストを出し8位入賞を果たした鈴木明子は「無事に終わってホッとしています。想像していた通り、緊張するだろうなと思って入ってきていたので、予想通りすごい緊張感はあったんですけど、会場の空気感を目で見て、耳で感じて、肌でも感じられるようにしたいなと思って、そこから滑り出したいと思っていた」と振り返った。  また、日本女子フィギュア勢で唯一トリノ五輪を経験している安藤は、前大会と比較したプレッシャーの差を訊かれると、「日本の人の声は気になったり、今でも怖い。どうやって言われるだろうとか。でも、そういうものを全日本で出してしまってジャンプの失敗に繋がったので、今回は一つの作品として、とにかく難易度を下げてでも一つの作品として滑りたかった」と明かし、そのプレッシャーの重さを感じさせた。\n",
      "sports\n"
     ]
    }
   ],
   "source": [
    "data = [l.strip().split('\\t') for l in open('train.tsv')]\n",
    "sents, labels = list(zip(*data))\n",
    "print(sents[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "tokenized_sents = [tagger.parse(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=2, max_df=0.5, max_features=1000)\n",
    "tfs = vectorizer.fit_transform(tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '01', '05', '06', '10', '100', '1000', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '200', '2009', '2010', '2011', '2012', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '300', '31', '32', '35', '40', '48', '50', '500', '60', 'akb', 'android', 'arrows', 'au', 'cm', 'com', 'cpu', 'facebook', 'fi', 'galaxy', 'gb', 'ghz', 'google', 'hd', 'htc', 'http', 'ipad', 'iphone', 'is', 'isw', 'it', 'jp', 'livedoor', 'lte', 'mah', 'max', 'mbps', 'mm', 'movie', 'news', 'ntt', 'on', 'one', 'optimus', 'os', 'pc', 'phone', 'play', 'salon', 'sh', 'smaxjp', 'softbank', 'sports', 'store', 'sx', 'tbs', 'the', 'tv', 'twitter', 'watch', 'web', 'wi', 'xi', 'xperia', 'ああ', 'あげ', 'あっ', 'あと', 'あなた', 'あの', 'あまり', 'あり', 'あれ', 'いい', 'いう', 'いえ', 'いか', 'いかが', 'いき', 'いく', 'いけ', 'いただき', 'いっ', 'いつ', 'いつも', 'いろいろ', 'うち', 'うまく', 'おく', 'おすすめ', 'および', 'おり', 'お知らせ', 'お金', 'かけ', 'かなり', 'かも', 'かわいい', 'ください', 'くらい', 'くる', 'くれ', 'くれる', 'けど', 'こう', 'ここ', 'こそ', 'こちら', 'この', 'これ', 'これから', 'こんな', 'ござい', 'ごと', 'さまざま', 'さらに', 'さん', 'しか', 'しかし', 'しかも', 'しっかり', 'しまい', 'しまう', 'しまっ', 'しよ', 'しれ', 'じゃ', 'すぎ', 'すぎる', 'すぐ', 'すごい', 'すごく', 'すでに', 'すべて', 'すれ', 'ずっと', 'せる', 'そう', 'そういう', 'そこ', 'そこで', 'そして', 'そのまま', 'その他', 'その後', 'それ', 'それぞれ', 'それでも', 'そんな', 'たい', 'たく', 'たくさん', 'ただ', 'たち', 'ため', 'たら', 'たり', 'だから', 'だが', 'だけ', 'だっ', 'だって', 'だろ', 'ちゃ', 'ちゃう', 'ちゃっ', 'ちゃん', 'ちょっと', 'って', 'っていう', 'つい', 'つけ', 'つつ', 'てる', 'でき', 'できる', 'でし', 'でしょ', 'でも', 'といった', 'とか', 'とき', 'ところ', 'として', 'とても', 'とにかく', 'とも', 'どう', 'どこ', 'どちら', 'どの', 'どれ', 'どんな', 'なお', 'なかっ', 'なかなか', 'ながら', 'なきゃ', 'なく', 'なけれ', 'なし', 'なぜ', 'なでしこ', 'など', 'なら', 'なり', 'なれ', 'なん', 'なんか', 'なんて', 'なんと', 'において', 'における', 'について', 'にて', 'にとって', 'によって', 'により', 'による', 'に対し', 'に対して', 'に対する', 'に関して', 'に関する', 'ので', 'のに', 'のみ', 'はじめ', 'はず', 'ばかり', 'ひとつ', 'ぶり', 'べき', 'ほう', 'ほか', 'ほしい', 'ほとんど', 'ほど', 'まあ', 'まさに', 'まし', 'ましょ', 'まず', 'まずは', 'ませ', 'また', 'まだ', 'まで', 'まとめ', 'まま', 'みたい', 'みなさん', 'みよ', 'みる', 'みんな', 'もう', 'もちろん', 'もっと', 'もの', 'ものの', 'もらっ', 'もん', 'やすい', 'やすく', 'やっ', 'やっぱり', 'やはり', 'やり', 'やる', 'よい', 'よう', 'よかっ', 'よく', 'より', 'よる', 'らしい', 'られ', 'られる', 'れる', 'わから', 'わかる', 'わけ', 'んで', 'アイテム', 'アクション', 'アサヒ', 'アジア', 'アスリート', 'アップ', 'アップデート', 'アナ', 'アプリ', 'アメリカ', 'アーティスト', 'イベント', 'イメージ', 'インタビュー', 'インターネット', 'インチ', 'インテル', 'エスマックス', 'エース', 'オススメ', 'オリンピック', 'オープン', 'カズ', 'カップ', 'カバー', 'カメラ', 'カラー', 'カード', 'ガール', 'キャリア', 'キャンペーン', 'キー', 'ギャラリー', 'クラブ', 'クリスマス', 'クリック', 'グループ', 'ケーキ', 'ケース', 'ゲスト', 'ゲーム', 'コア', 'コピー', 'コメント', 'コンテンツ', 'コース', 'コーチ', 'コード', 'ゴルフ', 'ゴール', 'サイズ', 'サイト', 'サッカー', 'サポート', 'サービス', 'システム', 'ショップ', 'シリーズ', 'シンプル', 'シーズン', 'シーン', 'ジャパン', 'スタンド', 'スタート', 'ストーリー', 'スポーツ', 'スマ', 'スマホ', 'スマート', 'スマートフォン', 'セット', 'ゼロ', 'ソフトウェア', 'ソフトバンク', 'タイプ', 'タグ', 'タッチ', 'タップ', 'タブレット', 'タレント', 'ダウンロード', 'ダメ', 'ダルビッシュ', 'チェック', 'チャンス', 'チーム', 'ツイッター', 'ツイート', 'テレビ', 'テーマ', 'ディスプレイ', 'デザイン', 'デジ', 'デジタル', 'データ', 'トップ', 'ドイツ', 'ドコモ', 'ドラマ', 'ニコニコ', 'ニュース', 'ネット', 'ノム', 'ノート', 'バッテリー', 'バランス', 'バージョン', 'パソコン', 'パフォーマンス', 'ヒット', 'ビデオ', 'ピッチャー', 'ファイル', 'ファッション', 'ファン', 'フォト', 'フォン', 'フジテレビ', 'フランス', 'フリー', 'ブック', 'ブランド', 'ブログ', 'プラチナ', 'プラン', 'プレゼント', 'プレッシャー', 'プレー', 'プロ', 'ベスト', 'ページ', 'ホーム', 'ボタン', 'ボール', 'ポイント', 'マン', 'マーク', 'メイク', 'メイン', 'メディア', 'メニュー', 'メンバー', 'メーカー', 'メール', 'モデル', 'モバイル', 'ユーザー', 'ライト', 'ライブ', 'ライブドアブログ', 'ライン', 'ランキング', 'リリース', 'リンク', 'リーグ', 'レビュー', 'レベル', 'レポート', 'ロンドン', 'ワンセグ', '一つ', '一方', '一番', '一緒', '一部', '上げ', '不安', '不思議', '与え', '世代', '世界', '中国', '中心', '中日', '主演', '予告', '予定', '予想', '予約', '予選', '事件', '事前', '事実', '五輪', '人気', '人生', '人間', '今回', '今夏', '今年', '今後', '今日', '今月', '仕事', '仕様', '付き', '付け', '代表', '以上', '以下', '以前', '以外', '以降', '企業', '企画', '会場', '会社', '会見', '会議', '会長', '伝え', '体験', '作っ', '作品', '使い', '使う', '使える', '使っ', '使用', '価値', '価格', '便利', '俳優', '個人', '優勝', '充電', '先生', '入っ', '入り', '入る', '入れ', '入力', '全く', '全て', '全国', '全然', '全部', '公式', '公開', '内容', '再生', '写真', '凄い', '出し', '出す', '出る', '出場', '出演', '出身', '分から', '初めて', '判断', '利用', '制作', '前田', '加え', '効果', '動き', '動作', '動画', '勝利', '募集', '北海道', '協会', '印象', '原因', '厳しい', '参加', '友人', '友達', '反応', '取材', '受け', '受信', '受賞', '可愛い', '可能', '史上', '合わせ', '吉田', '同じ', '同士', '同時に', '同誌', '名前', '向け', '含め', '周り', '周囲', '呼ば', '商品', '問題', '回答', '国内', '国際', '地域', '執筆', '基本', '報じ', '報道', '場合', '場所', '増え', '壁紙', '売れ筋', '変え', '変わっ', '変化', '変更', '多い', '多かっ', '多く', '大きく', '大きな', '大事', '大人', '大会', '大切', '大変', '大好き', '大学', '大阪', '夫婦', '契約', '女優', '女子', '女性', '女王', '好き', '始め', '始める', '姿勢', '嬉しい', '子ども', '子供', '存在', '安藤', '完全', '実は', '実力', '実施', '実現', '実際', '家族', '容量', '寄せ', '対応', '対戦', '対策', '対象', '専用', '少し', '少ない', '展示', '展開', '岡田', '巨人', '市場', '平均', '年間', '年齢', '幸せ', '広島', '引退', '強い', '強く', '当時', '当選', '影響', '彼女', '彼氏', '後半', '得点', '心配', '必要', '応募', '応援', '怒り', '思い', '思う', '思っ', '思わ', '恋愛', '悩み', '悪い', '情報', '意味', '意見', '意識', '感じ', '感じる', '成功', '成長', '所属', '批判', '技術', '投げ', '投手', '披露', '担当', '持っ', '持つ', '指摘', '挑戦', '挙げ', '振り返っ', '振り返る', '採用', '探し', '接続', '掲示板', '掲載', '提供', '搭載', '携帯', '撮影', '操作', '改めて', '攻撃', '放送', '教え', '文字', '斎藤', '料理', '新しい', '新た', '方法', '旅行', '日本', '日本テレビ', '日本人', '早く', '明かし', '明かす', '映像', '映画', '昨年', '時代', '時期', '時間', '普通', '更新', '書い', '最も', '最初', '最大', '最強', '最後', '最新', '最終', '最近', '最高', '有効', '有名', '期待', '期間', '本人', '本体', '本当に', '本田', '条件', '東京', '松井', '果たし', '柔道', '栄子', '検索', '楽しい', '楽しみ', '楽しめる', '楽しん', '楽天', '様々', '様子', '模様', '横浜', '機器', '機種', '機能', '欲しい', '歴史', '毎日', '気持ち', '決め', '決勝', '決定', '注意', '注目', '活動', '活用', '活躍', '流れ', '浅田', '海外', '液晶', '深夜', '測定', '満足', '演じる', '無料', '特に', '特別', '特徴', '特集', '状態', '状況', '独身', '獲得', '現在', '現地', '現場', '現役', '球団', '理由', '環境', '生まれ', '生活', '用意', '田中', '男子', '男性', '画像', '画素', '画面', '番組', '発売', '発表', '発言', '登場', '登録', '監督', '目標', '相手', '相談', '瞬間', '知っ', '知ら', '研究', '確か', '確認', '社会', '社長', '移籍', '種類', '端子', '端末', '競技', '笑顔', '答え', '管理', '簡単', '紹介', '終わっ', '終了', '経験', '結婚', '結果', '絵文字', '絶対', '絶賛', '続く', '続け', '続ける', '総合', '編集', '練習', '美しい', '美人', '美女', '美容', '考え', '考える', '聞い', '聞か', '自ら', '自信', '自分', '自動', '自殺', '自由', '自身', '自転車', '興味', '舞台', '良い', '芸能', '若い', '苦笑い', '落合', '行う', '行く', '行っ', '行わ', '行動', '表示', '製品', '見え', '見せ', '見せる', '見る', '視聴', '解説', '言い', '言う', '言え', '言っ', '言わ', '言葉', '訊か', '記事', '記念', '記者', '記録', '設定', '評価', '試し', '試合', '話し', '話題', '詳細', '語っ', '語り', '語る', '説明', '調査', '負け', '販売', '質問', '購入', '身体', '転職', '近い', '述べ', '追加', '通り', '通信', '連絡', '連続', '週刊', '週間', '過ぎ', '過去', '違い', '違う', '選ぶ', '選手', '選手権', '選択', '部分', '部屋', '配信', '重要', '野村', '野球', '金メダル', '長友', '長谷部', '開催', '開始', '開幕', '開発', '間違い', '関係', '関連', '阪神', '防水', '限定', '集め', '集中', '離婚', '雰囲気', '電子', '電池', '電話', '非常', '韓国', '音楽', '頑張っ', '食べ', '食事', '香川', '高い', '高校', '高速', '魅力']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 84)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 651)\t4\n",
      "  (0, 743)\t3\n",
      "  (0, 188)\t2\n",
      "  (0, 210)\t5\n",
      "  (0, 744)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 734)\t1\n",
      "  (0, 640)\t2\n",
      "  (0, 805)\t1\n",
      "  (0, 562)\t1\n",
      "  (0, 898)\t2\n",
      "  (0, 935)\t1\n",
      "  (0, 846)\t1\n",
      "  (0, 192)\t2\n",
      "  (0, 256)\t3\n",
      "  (0, 135)\t2\n",
      "  (0, 335)\t1\n",
      "  (0, 230)\t1\n",
      "  (0, 676)\t2\n",
      "  (0, 208)\t1\n",
      "  (0, 296)\t2\n",
      "  (0, 303)\t1\n",
      "  :\t:\n",
      "  (0, 520)\t1\n",
      "  (0, 704)\t3\n",
      "  (0, 309)\t1\n",
      "  (0, 302)\t1\n",
      "  (0, 181)\t2\n",
      "  (0, 171)\t1\n",
      "  (0, 720)\t1\n",
      "  (0, 277)\t1\n",
      "  (0, 498)\t1\n",
      "  (0, 869)\t1\n",
      "  (0, 631)\t1\n",
      "  (0, 441)\t2\n",
      "  (0, 923)\t1\n",
      "  (0, 310)\t2\n",
      "  (0, 219)\t1\n",
      "  (0, 921)\t1\n",
      "  (0, 170)\t1\n",
      "  (0, 290)\t1\n",
      "  (0, 155)\t1\n",
      "  (0, 502)\t1\n",
      "  (0, 475)\t2\n",
      "  (0, 528)\t2\n",
      "  (0, 215)\t2\n",
      "  (0, 217)\t1\n",
      "  (0, 747)\t1\n"
     ]
    }
   ],
   "source": [
    "print(tfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwakaba/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(tfs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータの読み込みと予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'others' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'sports' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'sports' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others']\n"
     ]
    }
   ],
   "source": [
    "data = [l.strip().split('\\t') for l in open('test.tsv')]\n",
    "test_sents, test_labels = list(zip(*data))\n",
    "tokenized_test_sents = [tagger.parse(sent) for sent in test_sents]\n",
    "test_tfs = vectorizer.transform(tokenized_test_sents)\n",
    "predict_labels = lr.predict(test_tfs)\n",
    "print(predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(test_labels == predict_labels) / len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, max_features=1000)\n",
    "tfidfs = vectorizer.fit_transform(tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 747)\t0.07380085127830598\n",
      "  (0, 217)\t0.09993823831728248\n",
      "  (0, 215)\t0.08959786816243354\n",
      "  (0, 528)\t0.17795114504550474\n",
      "  (0, 475)\t0.18418028856411983\n",
      "  (0, 502)\t0.05676255456918252\n",
      "  (0, 155)\t0.07027789153819866\n",
      "  (0, 290)\t0.049124751732099466\n",
      "  (0, 170)\t0.07256645527774962\n",
      "  (0, 921)\t0.07027789153819866\n",
      "  (0, 219)\t0.06286989812051866\n",
      "  (0, 310)\t0.07915914379680627\n",
      "  (0, 923)\t0.08802341416663526\n",
      "  (0, 441)\t0.20479146898451642\n",
      "  (0, 631)\t0.07825714603561588\n",
      "  (0, 869)\t0.07984446861632172\n",
      "  (0, 498)\t0.07457411460341987\n",
      "  (0, 277)\t0.041849460743490344\n",
      "  (0, 720)\t0.08802341416663526\n",
      "  (0, 171)\t0.07735414394791652\n",
      "  (0, 181)\t0.09356391980772065\n",
      "  (0, 302)\t0.03975560775837314\n",
      "  (0, 309)\t0.062243695658803125\n",
      "  (0, 704)\t0.18445160456104961\n",
      "  (0, 520)\t0.09381868610786533\n",
      "  :\t:\n",
      "  (0, 303)\t0.09634272909183711\n",
      "  (0, 296)\t0.12479735446107872\n",
      "  (0, 208)\t0.07256645527774962\n",
      "  (0, 676)\t0.20655202816707366\n",
      "  (0, 230)\t0.049124751732099466\n",
      "  (0, 335)\t0.10327601408353683\n",
      "  (0, 135)\t0.10004392782534868\n",
      "  (0, 256)\t0.14277577393737367\n",
      "  (0, 192)\t0.09003865205413544\n",
      "  (0, 846)\t0.09771516261531785\n",
      "  (0, 935)\t0.0657364529967937\n",
      "  (0, 898)\t0.18645631466505913\n",
      "  (0, 562)\t0.07329922676803494\n",
      "  (0, 805)\t0.07856608619096377\n",
      "  (0, 640)\t0.12902631560221994\n",
      "  (0, 734)\t0.05367853244191899\n",
      "  (0, 29)\t0.0862283255318338\n",
      "  (0, 20)\t0.08580033846467064\n",
      "  (0, 744)\t0.07795224442944808\n",
      "  (0, 210)\t0.25335600299511185\n",
      "  (0, 188)\t0.11895858377669168\n",
      "  (0, 743)\t0.12871158394560986\n",
      "  (0, 651)\t0.5120818857134164\n",
      "  (0, 91)\t0.0531496063592005\n",
      "  (0, 84)\t0.052327507493352174\n"
     ]
    }
   ],
   "source": [
    "print(tfidfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwakaba/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(tfidfs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'others' 'sports'\n",
      " 'sports' 'others' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports' 'sports'\n",
      " 'sports' 'sports' 'sports' 'sports' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'sports' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'sports' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'sports' 'others' 'others' 'others' 'others' 'others' 'others'\n",
      " 'others' 'others' 'others' 'others' 'others' 'others' 'others' 'others']\n"
     ]
    }
   ],
   "source": [
    "data = [l.strip().split('\\t') for l in open('test.tsv')]\n",
    "test_sents, test_labels = list(zip(*data))\n",
    "tokenized_test_sents = [tagger.parse(sent) for sent in test_sents]\n",
    "test_tfs = vectorizer.transform(tokenized_test_sents)\n",
    "predict_labels = lr.predict(test_tfs)\n",
    "print(predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(test_labels == predict_labels) / len(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
